{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python Spark SQL libraries\n",
    "# ALS is reccomendation class of the Spark MLlib built on Spark\n",
    "# Lit lets you put a contant value in the rows\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "# The function dictMovieNames just creates a Python \"dictionary\" we can later\n",
    "# use to convert movie ID's to movie names while printing out the final results\n",
    "def dictMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"ml-100k/u.item\") as i:\n",
    "        for line in i:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1].decode('ascii', 'ignore')\n",
    "    return movieNames\n",
    "\n",
    "\n",
    "# Convert the u.data lines into (userID, movieID, rating) rows\n",
    "def splitInput(line):\n",
    "    fields = line.value.split()\n",
    "    return Row(userID = int(fields[0]), movieID = int(fields[1]), rating = float(fields[2]))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder.appName(\"MovieRecs\").getOrCreate()\n",
    "\n",
    "    # Assign object with dictionary (movieID,movie name)\n",
    "    movieNames = dictMovieNames()\n",
    "\n",
    "    # Load the raw data and convert into rdd using .rdd\n",
    "    lines = spark.read.text(\"hdfs:///user/maria_dev/ml-100k/u.data\").rdd\n",
    "\n",
    "    # Convert it to a RDD of Row objects with (userID, movieID, rating)\n",
    "    ratingsRDD = lines.map(splitInput)\n",
    "\n",
    "    # Convert to a DataFrame and cache it as I will be using the DataFrame more than once\n",
    "    ratings = spark.createDataFrame(ratingsRDD).cache()\n",
    "\n",
    "    # Create an ALS collaborative filtering model from the complete data set\n",
    "    als = ALS(maxIter=5, regParam=0.01, userCol=\"userID\", itemCol=\"movieID\", ratingCol=\"rating\")\n",
    "    \n",
    "    # Tains the dataset on the above model and obtains the result\n",
    "    model = als.fit(ratings)\n",
    "    \n",
    "    # Print out ratings from user 0:\n",
    "    print(\"\\nRatings for user ID 0:\")\n",
    "    userRatings = ratings.filter(\"userID = 0\")\n",
    "    for rating in userRatings.collect():\n",
    "        print movieNames[rating['movieID']], rating['rating']\n",
    "\n",
    "    print(\"\\nTop 20 recommendations:\")\n",
    "    # Find movies rated more than 100 times\n",
    "    ratingCounts = ratings.groupBy(\"movieID\").count().filter(\"count > 100\")\n",
    "    # Construct a \"test\" dataframe for user 0 with every movie rated more than 100 times\n",
    "    popularMovies = ratingCounts.select(\"movieID\").withColumn('userID', lit(0))\n",
    "\n",
    "    # Run our model on that list of popular movies for user ID 0\n",
    "    recommendations = model.transform(popularMovies)\n",
    "\n",
    "    # Get the top 20 movies with the highest predicted rating for this user\n",
    "    topRecommendations = recommendations.sort(recommendations.prediction.desc()).take(20)\n",
    "\n",
    "    for recommendation in topRecommendations:\n",
    "        print (movieNames[recommendation['movieID']], recommendation['prediction'])\n",
    "\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
